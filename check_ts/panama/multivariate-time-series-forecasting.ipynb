{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Timeseries Multivariant Forecasting with LSTM model\n",
    "\n",
    "This notebook is about forecasting of the timeseries Load data avaialable in Train.csv.\n",
    "The change in Load is depend on the multiple variables like Temp, Humidity, Wind and Precipitation.\n",
    "\n",
    "We will train a LSTM model, which will take  hourly records available from 2015 to 2019.\n",
    "Then, we will predict the future demand for Jan-2020 and Compare with actual data that how model has worked..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('/kaggle/input/panama-electricity-load-forecasting/train.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:09.163044Z",
     "iopub.execute_input": "2021-11-16T18:14:09.163848Z",
     "iopub.status.idle": "2021-11-16T18:14:09.300472Z",
     "shell.execute_reply.started": "2021-11-16T18:14:09.163804Z",
     "shell.execute_reply": "2021-11-16T18:14:09.299139Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dropout, Flatten"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:09.30338Z",
     "iopub.execute_input": "2021-11-16T18:14:09.304029Z",
     "iopub.status.idle": "2021-11-16T18:14:09.315086Z",
     "shell.execute_reply.started": "2021-11-16T18:14:09.303984Z",
     "shell.execute_reply": "2021-11-16T18:14:09.313713Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:09.317888Z",
     "iopub.execute_input": "2021-11-16T18:14:09.318844Z",
     "iopub.status.idle": "2021-11-16T18:14:09.352888Z",
     "shell.execute_reply.started": "2021-11-16T18:14:09.318782Z",
     "shell.execute_reply": "2021-11-16T18:14:09.351148Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(df.dtypes)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:09.354963Z",
     "iopub.execute_input": "2021-11-16T18:14:09.355501Z",
     "iopub.status.idle": "2021-11-16T18:14:09.367986Z",
     "shell.execute_reply.started": "2021-11-16T18:14:09.355457Z",
     "shell.execute_reply": "2021-11-16T18:14:09.366552Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that there is no NULL values..Great...!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:09.370244Z",
     "iopub.execute_input": "2021-11-16T18:14:09.371713Z",
     "iopub.status.idle": "2021-11-16T18:14:09.390081Z",
     "shell.execute_reply.started": "2021-11-16T18:14:09.371661Z",
     "shell.execute_reply": "2021-11-16T18:14:09.388822Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From above- There are not Categorical dataset only Numerical dataset.\n",
    "But datetime column data type is object. Needs to be change"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df['datetime']=pd.to_datetime(df['datetime'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:09.391759Z",
     "iopub.execute_input": "2021-11-16T18:14:09.392414Z",
     "iopub.status.idle": "2021-11-16T18:14:14.814052Z",
     "shell.execute_reply.started": "2021-11-16T18:14:09.392373Z",
     "shell.execute_reply": "2021-11-16T18:14:14.812999Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:14.817931Z",
     "iopub.execute_input": "2021-11-16T18:14:14.818457Z",
     "iopub.status.idle": "2021-11-16T18:14:14.84974Z",
     "shell.execute_reply.started": "2021-11-16T18:14:14.818415Z",
     "shell.execute_reply": "2021-11-16T18:14:14.84876Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "datetime columne is directly can not be used for the Modem building but the we can create a features that repeated in nature any give that to the model so model can identify the pattern related to that features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#from datetime import datetime\n",
    "df['week_day']=df['datetime'].dt.dayofweek\n",
    "df['date']=df['datetime'].dt.day\n",
    "df['month']=df['datetime'].dt.month\n",
    "df['hour']=df['datetime'].dt.hour\n",
    "df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:14.852106Z",
     "iopub.execute_input": "2021-11-16T18:14:14.852533Z",
     "iopub.status.idle": "2021-11-16T18:14:14.905664Z",
     "shell.execute_reply.started": "2021-11-16T18:14:14.852471Z",
     "shell.execute_reply": "2021-11-16T18:14:14.90476Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#pip install autoviz"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:14.907538Z",
     "iopub.execute_input": "2021-11-16T18:14:14.908141Z",
     "iopub.status.idle": "2021-11-16T18:14:14.912592Z",
     "shell.execute_reply.started": "2021-11-16T18:14:14.908099Z",
     "shell.execute_reply": "2021-11-16T18:14:14.911424Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#from autoviz.AutoViz_Class import AutoViz_Class\n",
    "#AV = AutoViz_Class()\n",
    "#d = AV.AutoViz(df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:14.914434Z",
     "iopub.execute_input": "2021-11-16T18:14:14.915101Z",
     "iopub.status.idle": "2021-11-16T18:14:14.923648Z",
     "shell.execute_reply.started": "2021-11-16T18:14:14.915056Z",
     "shell.execute_reply": "2021-11-16T18:14:14.922465Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA Observations\n",
    "Almost all variables are following the Gaussian curve - No need to modify the parameters further\n",
    "Temp has the Maximum relationship with the required prediction 'nat_demand'\n",
    "'Holiday_ID' and 'Holiday' have the negative impact on the predictions values. We may neglect them if our model is not performing well.\n",
    "Average 'nat_demand' on Holidays are less than Normal days.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the 'nat_demand' with the short time frame...."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(30, 6))\n",
    "ddd=df[0:15000]\n",
    "ddd.groupby('datetime')['nat_demand'].median().plot()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:14.925185Z",
     "iopub.execute_input": "2021-11-16T18:14:14.925755Z",
     "iopub.status.idle": "2021-11-16T18:14:15.336762Z",
     "shell.execute_reply.started": "2021-11-16T18:14:14.925706Z",
     "shell.execute_reply": "2021-11-16T18:14:15.335422Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "I think we have reasonable insights to create a model. You may further to the EDA to find more insghts if you want.."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def add_features(df):\n",
    "    df['T2M_toc_s']=df['T2M_toc'].shift(-1).fillna(0)\n",
    "    df['QV2M_toc_s']=df['QV2M_toc'].shift(-1).fillna(0)\n",
    "    df['TQL_toc_s']=df['TQL_toc'].shift(-1).fillna(0)\n",
    "    df['W2M_toc_s']=df['W2M_toc'].shift(-1).fillna(0)\n",
    "    df['T2M_toc_s']=df['T2M_san'].shift(-1).fillna(0)\n",
    "    df['QV2M_san_s']=df['QV2M_san'].shift(-1).fillna(0)\n",
    "    df['TQL_san_s']=df['TQL_san'].shift(-1).fillna(0)\n",
    "    df['W2M_san_s']=df['W2M_san'].shift(-1).fillna(0)\n",
    "    df['T2M_dav_s']=df['T2M_dav'].shift(-1).fillna(0)\n",
    "    df['QV2M_dav_s']=df['QV2M_dav'].shift(-1).fillna(0)\n",
    "    df['TQL_dav_s']=df['TQL_dav'].shift(-1).fillna(0)\n",
    "    df['W2M_dav_s']=df['W2M_dav'].shift(-1).fillna(0)\n",
    "    df['Holiday_ID_s']=df['Holiday_ID'].shift(-1).fillna(0)\n",
    "    df['holiday_s']=df['holiday'].shift(-1).fillna(0)\n",
    "    df['school_s']=df['school'].shift(-1).fillna(0)\n",
    "    \n",
    "    df['T2M_toc_s1']=df['T2M_toc'].shift(-2).fillna(0)\n",
    "    df['QV2M_toc_s1']=df['QV2M_toc'].shift(-2).fillna(0)\n",
    "    df['TQL_toc_s1']=df['TQL_toc'].shift(-2).fillna(0)\n",
    "    df['W2M_toc_s1']=df['W2M_toc'].shift(-2).fillna(0)\n",
    "    df['T2M_toc_s1']=df['T2M_san'].shift(-2).fillna(0)\n",
    "    df['QV2M_san_s1']=df['QV2M_san'].shift(-2).fillna(0)\n",
    "    df['TQL_san_s1']=df['TQL_san'].shift(-2).fillna(0)\n",
    "    df['W2M_san_s1']=df['W2M_san'].shift(-2).fillna(0)\n",
    "    df['T2M_dav_s1']=df['T2M_dav'].shift(-2).fillna(0)\n",
    "    df['QV2M_dav_s1']=df['QV2M_dav'].shift(-2).fillna(0)\n",
    "    df['TQL_dav_s1']=df['TQL_dav'].shift(-2).fillna(0)\n",
    "    df['W2M_dav_s1']=df['W2M_dav'].shift(-2).fillna(0)\n",
    "    \n",
    "    df['nat_demand3']=df['nat_demand'].shift(3).fillna(0)\n",
    "    df['nat_demand4']=df['nat_demand'].shift(4).fillna(0)\n",
    "    df['nat_demand5']=df['nat_demand'].shift(5).fillna(0)\n",
    "    df['nat_demand6']=df['nat_demand'].shift(6).fillna(0)\n",
    "    df['nat_demand7']=df['nat_demand'].shift(7).fillna(0)\n",
    "    df['nat_demand8']=df['nat_demand'].shift(8).fillna(0)\n",
    "    df['nat_demand9']=df['nat_demand'].shift(9).fillna(0)\n",
    "    df['nat_demand10']=df['nat_demand'].shift(10).fillna(0)\n",
    "    df['nat_demand11']=df['nat_demand'].shift(11).fillna(0)\n",
    "    df['nat_demand12']=df['nat_demand'].shift(12).fillna(0)\n",
    "    df['nat_demand13']=df['nat_demand'].shift(13).fillna(0)\n",
    "    df['nat_demand14']=df['nat_demand'].shift(14).fillna(0)\n",
    "    df['nat_demand_n']=df['nat_demand']  \n",
    "    #df = pd.get_dummies(df)\n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:15.338347Z",
     "iopub.execute_input": "2021-11-16T18:14:15.339161Z",
     "iopub.status.idle": "2021-11-16T18:14:15.364146Z",
     "shell.execute_reply.started": "2021-11-16T18:14:15.339119Z",
     "shell.execute_reply": "2021-11-16T18:14:15.362987Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df1 = add_features(df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:15.36609Z",
     "iopub.execute_input": "2021-11-16T18:14:15.366662Z",
     "iopub.status.idle": "2021-11-16T18:14:15.423276Z",
     "shell.execute_reply.started": "2021-11-16T18:14:15.366614Z",
     "shell.execute_reply": "2021-11-16T18:14:15.422163Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "col=['datetime','nat_demand']\n",
    "new_df= df1.drop(columns=col)\n",
    "df_for_training = new_df.astype(float)\n",
    "df_for_training.head(10)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:15.425047Z",
     "iopub.execute_input": "2021-11-16T18:14:15.42587Z",
     "iopub.status.idle": "2021-11-16T18:14:15.490887Z",
     "shell.execute_reply.started": "2021-11-16T18:14:15.425822Z",
     "shell.execute_reply": "2021-11-16T18:14:15.489618Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling of data is required...."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:15.492494Z",
     "iopub.execute_input": "2021-11-16T18:14:15.493159Z",
     "iopub.status.idle": "2021-11-16T18:14:15.532228Z",
     "shell.execute_reply.started": "2021-11-16T18:14:15.493111Z",
     "shell.execute_reply": "2021-11-16T18:14:15.531043Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_for_training_scaled"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:15.533866Z",
     "iopub.execute_input": "2021-11-16T18:14:15.534539Z",
     "iopub.status.idle": "2021-11-16T18:14:50.285748Z",
     "shell.execute_reply.started": "2021-11-16T18:14:15.534464Z",
     "shell.execute_reply": "2021-11-16T18:14:50.284575Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Most Important part...\n",
    "Now, for this perticular dataset the past values of the records are import to predict the future demand and the Model has to be dynamic to learn from past data and predict the future demand.\n",
    "\n",
    "- For this, we have to create parts of data having past 2 days (48 hours)of the data to give it the same to the LSTM model to predict the 49th Hour Load prediction.\n",
    "\n",
    "I hope I clear, if not please ask me in discussion I will clear your doubt. This is the most import part of the this Notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X=np.array(df_for_training)\n",
    "print(X)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:14:50.287503Z",
     "iopub.execute_input": "2021-11-16T18:14:50.288142Z",
     "iopub.status.idle": "2021-11-16T18:15:26.316455Z",
     "shell.execute_reply.started": "2021-11-16T18:14:50.288094Z",
     "shell.execute_reply": "2021-11-16T18:15:26.31335Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "trainY = df_for_training['nat_demand_n'].to_numpy().reshape(-1,1)\n",
    "trainX = df_for_training.to_numpy().reshape(-1, 57,1)\n",
    "#trainY = df[['pressure']].to_numpy().reshape(-1, 80)\n",
    "#trainX = df.reshape(-1, 80, df.shape[-1])      df_for_training.shape[-1]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:15:26.318074Z",
     "iopub.execute_input": "2021-11-16T18:15:26.318551Z",
     "iopub.status.idle": "2021-11-16T18:15:26.327805Z",
     "shell.execute_reply.started": "2021-11-16T18:15:26.318472Z",
     "shell.execute_reply": "2021-11-16T18:15:26.32635Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#trainX = []\n",
    "#trainY = []\n",
    "#n_future = 1 \n",
    "#n_past = 48\n",
    "#for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "#    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "#    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:15:26.329269Z",
     "iopub.execute_input": "2021-11-16T18:15:26.330415Z",
     "iopub.status.idle": "2021-11-16T18:15:26.352506Z",
     "shell.execute_reply.started": "2021-11-16T18:15:26.33027Z",
     "shell.execute_reply": "2021-11-16T18:15:26.350707Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert it to Numpy array....Think why >>>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:15:26.355214Z",
     "iopub.execute_input": "2021-11-16T18:15:26.356285Z",
     "iopub.status.idle": "2021-11-16T18:15:26.377131Z",
     "shell.execute_reply.started": "2021-11-16T18:15:26.356228Z",
     "shell.execute_reply": "2021-11-16T18:15:26.375957Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(threshold=2000)\n",
    "#p = PrintArray(precision=4, linewidth=150, suppress=True)\n",
    "trainX"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T18:15:26.378816Z",
     "iopub.execute_input": "2021-11-16T18:15:26.379442Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a LSTM Multivariant model \n",
    "\n",
    "Now, we have to create a multi stage Nuetral network -LSTM (Long Short Term Memory) moedel. \n",
    "The Modem will have the return sequence True to understand the sequncing of the data and identify the pattern. Hope it works the fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]),return_sequences=True)))\n",
    "                             #return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(32, activation='relu', return_sequences=True)))\n",
    "#model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "#model.add(Bidirectional(LSTM(24, activation='relu', return_sequences=False)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))#, return_sequences=False))\n",
    "#model.add(Dense(32, activation='relu'))#, return_sequences=False))\n",
    "model.add(Dense(8, activation='relu'))#, return_sequences=False))\n",
    "model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "#model.summary()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "history = model.fit(trainX, trainY, epochs=5, batch_size=200,verbose=1)\n",
    "#validation_split=0.2,\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "#plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model has performed well on validation data...\n",
    "Now, make ready the Test data set ready for prediction Jan-2020"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_test= pd.read_csv('/kaggle/input/panama-electricity-load-forecasting/Test_Jan.csv',encoding=\"latin-1\")\n",
    "df_test['datetime']=pd.to_datetime(df_test['datetime'])\n",
    "df_test['week_day']=df_test['datetime'].dt.dayofweek\n",
    "df_test['date']=df_test['datetime'].dt.day\n",
    "df_test['month']=df_test['datetime'].dt.month\n",
    "df_test['hour']=df_test['datetime'].dt.hour\n",
    "df_test = add_features(df_test)\n",
    "df_for_pred=df_test.drop(columns=col)\n",
    "df_for_pred.astype(float)\n",
    "df_pred_scaled=scaler.transform(df_for_pred)\n",
    "\n",
    "X_pred = []\n",
    "for i in range(n_past, len(df_pred_scaled) - n_future +1):\n",
    "    X_pred.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "X_pred= np.array(X_pred)\n",
    "\n",
    "print('X for prediction shape == {}.'.format(X_pred.shape))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make a Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, values are not in the format in which we required.\n",
    "We have to do the inverse Transform.."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "prediction = model.predict(X_pred) \n",
    "prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\n",
    "y_pred_future\n",
    "df_forecast=[]\n",
    "df_forecast = pd.DataFrame(y_pred_future)\n",
    "temp_df=[]\n",
    "temp_df=pd.DataFrame(np.zeros(n_past))\n",
    "df_forecast =pd.concat([temp_df,df_forecast], ignore_index=True)\n",
    "df_forecast\n",
    "df_actual= pd.read_csv('/kaggle/input/panama-electricity-load-forecasting/Predict_Jan.csv',encoding=\"latin-1\")\n",
    "df_actual['Pred_nat_demand']=df_forecast\n",
    "df_actual = df_actual.iloc[n_past:]\n",
    "df_actual\n",
    "a4_dims = (25,8)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.lineplot(df_actual['datetime'], df_actual['nat_demand'])\n",
    "sns.lineplot(df_actual['datetime'], df_actual['Pred_nat_demand'])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prediction = model.predict(X_pred) \n",
    "prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\n",
    "y_pred_future\n",
    "df_forecast=[]\n",
    "df_forecast = pd.DataFrame(y_pred_future)\n",
    "temp_df=[]\n",
    "temp_df=pd.DataFrame(np.zeros(n_past))\n",
    "df_forecast =pd.concat([temp_df,df_forecast], ignore_index=True)\n",
    "df_forecast\n",
    "df_actual= pd.read_csv('/kaggle/input/panama-electricity-load-forecasting/Predict_Jan.csv',encoding=\"latin-1\")\n",
    "df_actual['Pred_nat_demand']=df_forecast\n",
    "df_actual = df_actual.iloc[n_past:]\n",
    "df_actual\n",
    "a4_dims = (25,8)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.lineplot(df_actual['datetime'], df_actual['nat_demand'])\n",
    "sns.lineplot(df_actual['datetime'], df_actual['Pred_nat_demand'])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_forecast\n",
    "#df_forecast"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
