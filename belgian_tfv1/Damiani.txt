Lara (in cc) vorrebbe fare un po' di esperimenti per valutare il danno degli attacchi di append ai training set, in cui
l'attaccante aggiunge punti spuri (con valori random noise o generati da un algoritmo) a un training set.

Le ho consigliato di utilizzare l'esempio dei segnali stradali che abbiamo usato per il corso, anche perchè è un
classico per la letteratura del settore.

L'idea è partire dal training set che avevi convertito in grayscale per il corso (ti sarei grato se tu potessi inviarlo
a Lara, così ne ha copia) e togliere anzitutto 100 punti bilanciati per categoria da tenere da parte come Held-out set.

In parallelo, Lara prepara alcuni punti spuri e te li manda

Tu, se riesci, dovresti addestrare il modello due volte, una con il training set originale (ovviamente senza i punti
tenuti da parte come Held-out), una dopo aver aggiunto al training set i punti spuri che Lara ti ha mandato. Ovviamente
l'addestramento avviene in entrambi i casi con cross validation ecc.

Per entrambi i modelli, ci servirebbero poi le classificazioni fatte dal modello addestrato (non quelle che avevano)
dei punti Held-out.

Per tua info, il nostro programma è questo:  Usando i punti held-out come vettori e le loro classi di appartenenza
generate dai due modelli, Lara calcolerà approssimazioni degli inviluppi dei vettori di ciascuna classe per i due
modelli; l'obiettivo è definire una misura rapida dell'impatto dei punti spuri come deformazione degli inviluppi, e
usarla come misura di severità (diversa da quelle solite  che guardano lo spostamento delle superfici di separazione)