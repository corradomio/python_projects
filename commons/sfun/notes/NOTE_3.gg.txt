Qualche osservazione tra le righe stile
<GG blabla GG>



________________________________________
From: Corrado Mio [corrado.mio@unimi.it]
Sent: Sunday, December 02, 2018 11:19 AM
To: Gabriele Gianini
Subject: Re: approssimazioni

Ciao,

ho alcune ulteriori note', che potrebbero essere utili.


Prima parte
----------------------

1) la funzione da approssimare (la qualita' del risultato di un algo di ML in base al numero di feature selezionate) e' sicuramente MONOTONA,
    ma e' anche SUBADDITIVA, non additiva (troppo facile), ne superadditiva (troppa grazia)

<GG
concordo, possiamo assumere che tipicamente usando ll'accuratezza degli algoritmi di ML come funzione dei set di feature ,
la funzione pseudo-booleana prodotta sia MONOTONA e SUBADDITIVA
questo restringe notevolmente il campo delle funzioni pseudobooleane
(aggiungo anche che in generale non è ne submodulare ne supermodulare, non è cioè garantito un vantaggio nell'aggregarsi a
coalizioni piccole rispetto a quelle più grandi, ne il viceversa)
GG>


2) quando si usa una PERMUTAZIONE per valutare lo shapley value, stiamo anche valutando la funzione su tutti i sottoinsiemi generati dai
   PREFISSI della permutazione (gli elemnti dalla poizione 1 alla k, con k che va da 1 a n, n il numero totale di elementi)
   Tenere traccia di questi valori, potrebbe NON FUNZIONARE, perche' c'e' ne sarebbero troppi

   Spannometricamente:  per ogni permutazione di N elementi, abbiamo N sottoinsiemi, Se usiamo K permutazioni per fare delle approssimazioni,
   dovremmo tenere traccia di N*K sottoinsiemi.

   Vabbe' scritta cosi', forse non e' cosi' grave.


<GG
occorrerebbe individuare una struttra dati efficiente nel memorizzare e recuperare i dati
che sono punti del reticolo (anche con riferimento all'osservazione che fai tra parentesi qui sotto nella 3)
a tale proposito, nota che le permutazioni rappresentano delle catene complete lungo il reticolo
GG>



3) considerando i prefissi della permutazione, noi percorriamo il reticolo dal'insieme vuoto all'insieme al fullset.
   La prima cosa che possiamo notare e' che, poiche' la funzione e' SUBADDITIVA e noi conosciamo gia' il valore finale (quello sul fullset),
   ci potremmo fermare PRIMA, perche' l'apporto degli elementi mancanti, da un certo punto in poi, e' facilmente calcolabile.

   (Nota implementativa: se si salvano i risultati dell'algo di ML per un certo set, NON SERVE RIESEGUIRLO per lo stesso set!!!!)

4) l'altro modo di approssimare la funzione e' quello di fare del campionamento sui punti del reticolo. Qui' ci sarebbero almeno due meccanismi
    un campionamento REGOLARE dei vari livelli (quelli con K, K+1 ,... elementi in ogni insieme), oppure un campionamento RANDOM

4.1) qui si potrebbe aggiungere un qualche meccanismo che campiona di piu' le zone del reticolo che sembrano piu' "interessanti" (dove la
      funzione varia di piu'), In particolare, questo sistema prediligerebbe i livelli piu' bassi del reticolo, dove sappiamo gia' che si "annida" la
     maggior parte dell'informazione

<GG
precisamente: un campionamento adattativo
GG>


Seconda parte.
----------------------

Qualunque sistema di approssimazione venga selezionato, deve essere validato.

E la validazione non puo' che essere fatta usando delle set function che conosciamo perfettamente.

Il problema e' COME generare la funzione: servono delle funzioni che mimino il comportamento delle performace degli algo di ML.
Ma questo e' un'altro problema ancora.

Al momento abbiamo tre tipi di approssimazioni:  SV, SV+SIV, Interaction Index di K-mo grado. Ed altre possono arrivare nel futuro

Il meccanismo per validare l'approssimazione potrebbe essere questo:

generiamo un certo numero di funzioni  in modo random, e calcoliamo il mean square error (MSE), in base al numero di elemento del fullset .

Fino a 2^16 ci arriviamo. Si potrebbe arrivare anche fino a 2^20, ma ci vorrebbe un sacco di tempo di calcolo.

<GG
per una proof of concept puo' bastare 2^16
GG>



Terza parte:
------------------------

Identificazione della migliore partizione di 2 componenti.

Anche qui il meccanismo potrebbe essere questo: si cerca la VERA partizione in 2 componenti

Si calcola la partizione con i vari sistemi che avevo messo in piedi l'altra volta (ottimizzazione, l'algoritmo salterino che mi ero inventato, ...)
E si calcola il MSE

Poi bisogna applicarlo alla feature selection



Quarta parte
-------------------
Ovviamente bisognerebbe provare il tutto su un vero dataset.

Ma il nostro e' un META algoritmo, e quindi dell'algo di ML ci serve SOLO le sue performance su tutti i possibili subset di feature.

Qui non ci starebbe male subappaltare il lavoro a qualche "adepto"/"adepta" :-)



Problemi aperti
----------------------

I problemi aperti sarebbero:

1) come generare una funzione
2) come trovare una buona approssimazione

view splittinge e feature selection sarebbero una conseguenza



Per Passau
---------------------------

Che dici?
Anche solo a raccontare queste cose, con qualche risultato iniziale, dovrebbe andare bene.


<GG del contenuto di passau è meglio parlare a voce.... GG>


Nota
---------------
Le varie trasformate descritte in "2000 - Grabisch" sono presenti anche nel suo libro e, cosa
utile, ha anche normalizzato la simbologia. Alcune le ho implementate. Ci sono altre
ma bisogna vedere se sono utili per i nostri scopi.

Mi manca la rappresentazione stile "Hammer"

<GG buon lavoro allora ;-)  GG>



On 01/12/18 12:38, Gabriele Gianini <gabriele.gianini@ku.ac.ae> wrote:
Ho messo un file.txt nella cartella condivisa,
insieme ai pdf menzionati nel file.
Lo allego anche qui
g


________________________________________
From: Corrado Mio [corrado.mio@unimi.it]
Sent: Friday, November 30, 2018 10:25 AM
To: Gabriele Gianini
Subject: Shapley Interaction Transform e sua inversa - 3 parte

Ciao,
ti mando l'ultima lista di note, sui cui si puo' ragionare quando avrai tempo.

La considerazione finale e':

APPROSSIMARE una funzione usando lo SV piu' lo SIV ed usando la regola somma/sottrai

SEMBRA una BUONA idea.

La domanda sorge spontanea: PERCHE?

Non solo, ma anche:

QUANTO migliore e' l'approssimazione rispetto ad usare SOLO lo SV?

Ed anche: le approssimazioni sono buone/cattive SEMPRE nello stesso modo, oppure dipendono
pesantemente dalla funzione?

Fortunatamente nel nostro contesto abbiamo a che fare solo con fuzioni monotone (gli algo di ML MIGLIORANO
con l'aumento del numero di feature, non peggiorano mai).

Vabbe, la lista di domande c'e'.

Un articoletto ragionevolmente intelligente ci puo' anche uscire  :-)


NOTA DI SERVIZIO:

Alessandro mi ha detto che c'e' un po' di malumore per il fatto che vengo ad Abu Dhabi.
Chi e perche' non l'ho chiesto.
Prova a sentirlo tu, se non altro per capire QUANTO la cosa puo' esser grave.

Un modo per stemperare il problema potrebbe essere non dire a nessuno delle ulteriori missioni legate
al nuovo contratto (sempre se viene confermato).


Ciao




